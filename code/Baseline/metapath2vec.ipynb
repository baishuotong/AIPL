{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F73TxJ8JH8ki"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "import multiprocessing,os\n",
        "import pandas as pd\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph import datasets\n",
        "from stellargraph import IndexedArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSrVBtvgTg1p"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/VenkateshwaranB/stellargraph.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO4eiD-LN97R",
        "outputId": "3c0ad799-1cca-4003-c897-2afb35f58069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['pr', 'pr']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-e2127a85b666>:88: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  walks=np.array(walks)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['pr', 'repo', 'pr']]\n",
            "[['pr', 'user', 'pr']]\n",
            "[['pr', 'repo', 'repo', 'pr']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-e2127a85b666>:88: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  walks=np.array(walks)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['pr', 'repo', 'user', 'repo', 'pr']]\n",
            "[['pr', 'user', 'repo', 'user', 'pr']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-e2127a85b666>:88: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  walks=np.array(walks)\n"
          ]
        }
      ],
      "source": [
        "basic_repo='./facebook_react'\n",
        "\n",
        "users = pd.read_csv(basic_repo+'/index/user_index.txt', sep='\\t', header=None, names=['user_id', 'user'], keep_default_na=False, encoding='utf-8')\n",
        "users= list(map(lambda x:x, users['user_id'].tolist()))\n",
        "repos= pd.read_csv(basic_repo+'/index/repo_index.txt', sep='\\t', header=None, names=['repo_id', 'repo'], keep_default_na=False, encoding='utf-8')\n",
        "repos= list(map(lambda x:x+len(users), repos['repo_id'].tolist()))\n",
        "issues =pd.read_csv(basic_repo+'/index/issue_index.txt', sep='\\t', header=None, names=['issue_id', 'issue'], keep_default_na=False, encoding='utf-8')\n",
        "issues= list(map(lambda x:x+len(users)+len(repos), issues['issue_id'].tolist()))\n",
        "prs = pd.read_csv(basic_repo+'/index/pr_index.txt', sep='\\t', header=None, names=['pr_id', 'pr'], keep_default_na=False, encoding='utf-8')\n",
        "prs= list(map(lambda x:x+len(users)+len(repos)+len(issues), prs['pr_id'].tolist()))\n",
        "\n",
        "issues_ = IndexedArray(index=issues)\n",
        "prs_ = IndexedArray(index=prs)\n",
        "users_ = IndexedArray(index=users)\n",
        "repos_ = IndexedArray(index=repos)\n",
        "\n",
        "user_repo=pd.read_csv(basic_repo+'/edge_index/user_repo_index.txt', sep='\\t', header=None, names=['user_id', 'repo_id'], keep_default_na=False, encoding='utf-8')\n",
        "user_issue=pd.read_csv(basic_repo+'/edge_index/user_issue_index.txt', sep='\\t', header=None, names=['user_id', 'issue_id'], keep_default_na=False, encoding='utf-8')\n",
        "user_pr=pd.read_csv(basic_repo+'/edge_index/user_pr_index.txt', sep='\\t', header=None, names=['user_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "repo_repo = pd.read_csv(basic_repo+'/edge_index/repo_repo_index.txt', sep='\\t', header=None, names=['repo_id', 'repo_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "repo_issue = pd.read_csv(basic_repo+'/edge_index/repo_issue_index.txt', sep='\\t', header=None, names=['repo_id', 'issue_id'], keep_default_na=False, encoding='utf-8')\n",
        "repo_pr = pd.read_csv(basic_repo+'/edge_index/repo_pr_index.txt', sep='\\t', header=None, names=['repo_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "issue_issue=pd.read_csv(basic_repo+'/edge_index/issue_issue_index.txt', sep='\\t', header=None, names=['issue_id', 'issue_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "pr_pr=pd.read_csv(basic_repo+'/edge_index/pr_pr_index.txt', sep='\\t', header=None, names=['pr_id', 'pr_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "issue_pr=pd.read_csv(basic_repo+'/edge_index/issue_pr_index.txt', sep='\\t', header=None, names=['issue_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "edges_UR = pd.DataFrame({\"source\": user_repo['user_id'].tolist(), \"target\":list(map(lambda x:x+len(users),user_repo['repo_id'].tolist()))},index=list(range(0,len(user_repo))))\n",
        "edges_UI=  pd.DataFrame({\"source\": user_issue['user_id'].tolist(), \"target\":list(map(lambda x:x+len(users)+len(repos),user_issue['issue_id'].tolist()))},index=list(range(len(user_repo),len(user_repo)+len(user_issue))))\n",
        "edges_UP=  pd.DataFrame({\"source\": user_pr['user_id'].tolist(), \"target\":list(map(lambda x:x+len(users)+len(repos)+len(issues),user_pr['pr_id'].tolist()))},index=list(range(len(user_repo)+len(user_issue),len(user_repo)+len(user_issue)+len(user_pr))))\n",
        "\n",
        "edges_RR=  pd.DataFrame({\"source\":list(map(lambda x:x+len(users),repo_repo['repo_id'].tolist())), \"target\":list(map(lambda x:x+len(users),repo_repo['repo_id_1'].tolist()))},\n",
        "                        index=list(range(len(user_repo)+len(user_issue)+len(user_pr),len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo))))\n",
        "edges_RI=  pd.DataFrame({\"source\":list(map(lambda x:x+len(users),repo_issue['repo_id'].tolist())), \"target\":list(map(lambda x:x+len(users)+len(repos),repo_issue['issue_id'].tolist()))},\n",
        "                        index=list(range(len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo),len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue))))\n",
        "edges_RP=  pd.DataFrame({\"source\":list(map(lambda x:x+len(users),repo_pr['repo_id'].tolist())), \"target\":list(map(lambda x:x+len(users)+len(repos)+len(issues),repo_pr['pr_id'].tolist()))},\n",
        "                        index=list(range(len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue),len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue)+len(repo_pr))))\n",
        "edges_II=  pd.DataFrame({\"source\":list(map(lambda x:x+len(users)+len(repos),issue_issue['issue_id'].tolist())),\n",
        "                         \"target\":list(map(lambda x:x+len(users)+len(repos),issue_issue['issue_id_1'].tolist()))},\n",
        "                        index=list(range(len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue)+len(repo_pr),len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue)+len(repo_pr)+len(issue_issue))))\n",
        "edges_PP=  pd.DataFrame({\"source\":list(map(lambda x:x+len(users)+len(repos)+len(issues),pr_pr['pr_id'].tolist())),\"target\":list(map(lambda x:x+len(users)+len(repos)+len(issues),pr_pr['pr_id_1'].tolist()))},\n",
        "                        index=list(range(len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue)+len(repo_pr)+len(issue_issue),\n",
        "                                         len(user_repo)+len(user_issue)+len(user_pr)+len(repo_repo)+len(repo_issue)+len(repo_pr)+len(issue_issue)+len(pr_pr))))\n",
        "\n",
        "Graph = StellarGraph(nodes={\"issue\": issues_, \"pr\": prs_,\"user\": users_, \"repo\": repos_},\n",
        "                     edges={\"UR\":edges_UR,'UI':edges_UI,'UP':edges_UP,'RR':edges_RR,'RI':edges_RI,'RP':edges_RP,'II':edges_II,'PP':edges_PP})\n",
        "\n",
        "metapaths = [\n",
        "    [\n",
        "        [[\"issue\", \"issue\"]],\n",
        "    [ [\"issue\", \"repo\", \"issue\"]],\n",
        "    [ [\"issue\", \"user\", \"issue\"]],\n",
        "    [ [\"issue\", \"repo\", \"repo\", \"issue\"]],\n",
        "    [ [\"issue\", \"repo\", \"user\", \"repo\", \"issue\"]],\n",
        "    [ [\"issue\", \"user\", \"repo\",\"user\",\"issue\"]]\n",
        "     ],\n",
        "    [\n",
        "        [[\"pr\", \"pr\"]],\n",
        "    [[\"pr\", \"repo\", \"pr\"]],\n",
        "    [[\"pr\", \"user\", \"pr\"]],\n",
        "    [[\"pr\", \"repo\", \"repo\", \"pr\"]],\n",
        "    [[\"pr\", \"repo\", \"user\", \"repo\", \"pr\"]],\n",
        "    [[\"pr\", \"user\", \"repo\", \"user\", \"pr\"]]],\n",
        "]\n",
        "\n",
        "\n",
        "from stellargraph.data import UniformRandomMetaPathWalk\n",
        "\n",
        "# Create the random walker\n",
        "rw = UniformRandomMetaPathWalk(Graph)\n",
        "\n",
        "walk_length = 8\n",
        "\n",
        "issue_root_nodes=list(map(lambda x:x+len(users)+len(repos),issue_pr['issue_id'].tolist()))\n",
        "pr_root_nodes=list(map(lambda x:x+len(users)+len(repos)+len(issues),issue_pr['pr_id'].tolist()))\n",
        "i=0\n",
        "for metapath in metapaths[1]:\n",
        "  print(metapath)\n",
        "  i+=1\n",
        "  walks = rw.run(\n",
        "      nodes=list(pr_root_nodes),  # root nodes\n",
        "      length= walk_length,  # maximum length of a random walk\n",
        "      n=1,  # number of random walks per root node\n",
        "      metapaths=metapath,  # the metapaths\n",
        "  )\n",
        "\n",
        "  walks=np.array(walks)\n",
        "  np.save('/content/drive/MyDrive/facebook_react/metapath/pr/metapath_pr'+str(i)+'.npy',walks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EnnZgi6SFMT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "issue_walks=[]\n",
        "# pr_walks=[]\n",
        "\n",
        "# for index in os.listdir('/content/drive/MyDrive/facebook_react/metapath/pr/'):\n",
        "\n",
        "#   array_a=np.load('/content/drive/MyDrive/facebook_react/metapath/pr/'+index,allow_pickle=True)\n",
        "#   for i in array_a.tolist():\n",
        "#     pr_walks.append(i)\n",
        "# print(len(pr_walks))\n",
        "# str_pr_walks = [[str(n) for n in walk] for walk in pr_walks]\n",
        "\n",
        "# pr_model = Word2Vec(str_pr_walks, vector_size=64, window=5, min_count=0, sg=1, workers=2, epochs=100)\n",
        "# pr_model.save('/content/drive/MyDrive/facebook_react/metapath/pr/pr_word2vec.model')\n",
        "\n",
        "# vectors = pr_model.wv.vectors #获取模型中全部的节点向量\n",
        "# words = pr_model.wv.index_to_key #获取模型中全部的节点类型\n",
        "\n",
        "for index in os.listdir('/content/drive/MyDrive/facebook_react/metapath/issue/'):\n",
        "\n",
        "  array_a=np.load('/content/drive/MyDrive/facebook_react/metapath/issue/'+index,allow_pickle=True)\n",
        "  for i in array_a.tolist():\n",
        "    issue_walks.append(i)\n",
        "\n",
        "str_issue_walks = [[str(n) for n in walk] for walk in issue_walks]\n",
        "\n",
        "issue_model = Word2Vec(str_issue_walks, vector_size=64, window=5, min_count=0, sg=1, workers=2, epochs=100)\n",
        "issue_model.save('/content/drive/MyDrive/facebook_react/metapath/issue/issue_word2vec.model')\n",
        "\n",
        "vectors = issue_model.wv.vectors #获取模型中全部的节点向量\n",
        "words = issue_model.wv.index_to_key #获取模型中全部的节点类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3LsGyLrgBN8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8jGzHDnNNDE",
        "outputId": "2d41f344-9abf-4786-c0e0-37f98e45e9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "Link Prediction Tests Summary\n",
            "AUC_mean = 0.2999459167117361\n",
            "AP_mean = 0.7199124726477024\n",
            "AR_mean = 0.09572301425661914\n",
            "AF1_mean = 0.1689779147406266,\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "auc_list = []\n",
        "ap_list = []\n",
        "ar_list = []\n",
        "af1_list = []\n",
        "issue_model = Word2Vec.load('/content/drive/MyDrive/facebook_react/metapath/issue/issue_word2vec.model')\n",
        "pr_model = Word2Vec.load('/content/drive/MyDrive/facebook_react/metapath/pr/pr_word2vec.model')\n",
        "issue_vectors = issue_model.wv.vectors #获取模型中全部的节点向量\n",
        "issue_words = issue_model.wv.index_to_key #获取模型中全部的节点类型\n",
        "pr_vectors = pr_model.wv.vectors #获取模型中全部的节点向量\n",
        "pr_words = pr_model.wv.index_to_key #获取模型中全部的节点类型\n",
        "\n",
        "basic_repo='/content/drive/MyDrive/facebook_react'\n",
        "issue_pr=pd.read_csv(basic_repo+'/edge_index/issue_pr_index.txt', sep='\\t', header=None, names=['issue_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "train_val_test_pos_issue_pr = np.load(basic_repo + '/train_val_test_idx.npz')\n",
        "train_val_test_neg_issue_pr = np.load(basic_repo + '/train_val_test_neg_issue_pr.npz')\n",
        "\n",
        "train_pos_issue_pr = train_val_test_pos_issue_pr['train_pos_issue_pr']\n",
        "val_pos_issue_pr = train_val_test_pos_issue_pr['val_pos_issue_pr']\n",
        "test_pos_issue_pr = train_val_test_pos_issue_pr['test_pos_issue_pr']\n",
        "train_neg_issue_pr = train_val_test_neg_issue_pr['train_neg_issue_pr']\n",
        "val_neg_issue_pr = train_val_test_neg_issue_pr['val_neg_issue_pr']\n",
        "test_neg_issue_pr = train_val_test_neg_issue_pr['test_neg_issue_pr']\n",
        "\n",
        "predict=[]\n",
        "for index in train_pos_issue_pr:\n",
        "  pos_embedding_user=np.array([0]*64)\n",
        "  pos_embedding_artist=np.array([0]*64)\n",
        "  if str(index[0]) in issue_words:\n",
        "    pos_embedding_user=issue_model.wv[str(index[0])]\n",
        "  if str(index[1]) in pr_words:\n",
        "    pos_embedding_artist=pr_model.wv[str(index[1])]\n",
        "\n",
        "  # pos_embedding_user = pos_embedding_user.view(-1, 1, pos_embedding_user.shape[1])\n",
        "  # pos_embedding_artist = pos_embedding_artist.view(-1, pos_embedding_artist.shape[1], 1)\n",
        "  pos_out = np.dot(pos_embedding_user, pos_embedding_artist)\n",
        "  # pos_out = pos_out.squeeze(dim=-1).float()\n",
        "\n",
        "  predict.append(pos_out)\n",
        "\n",
        "for index in train_neg_issue_pr:\n",
        "  neg_embedding_user=np.array([0]*64)\n",
        "  neg_embedding_artist=np.array([0]*64)\n",
        "  if str(index[0]) in issue_words:\n",
        "    neg_embedding_user=issue_model.wv[str(index[0])]\n",
        "  if str(index[1]) in pr_words:\n",
        "    neg_embedding_artist=pr_model.wv[str(index[1])]\n",
        "\n",
        "  # neg_embedding_user = neg_embedding_user.view(-1, 1, neg_embedding_user.shape[1])\n",
        "  # neg_embedding_artist = neg_embedding_artist.view(-1, neg_embedding_artist.shape[1], 1)\n",
        "  neg_out = np.dot(neg_embedding_user, neg_embedding_artist)\n",
        "  # neg_out = neg_out.squeeze(dim=-1).float()\n",
        "  predict.append(neg_out)\n",
        "\n",
        "predict=np.array(predict)\n",
        "predict=np.where(predict>=0.5,1,0)\n",
        "y_true_test=np.array([1] * len(train_pos_issue_pr) + [0] * len(train_neg_issue_pr))\n",
        "\n",
        "auc= accuracy_score(y_true_test, predict.astype(int))\n",
        "precision= precision_score(y_true_test, predict)\n",
        "recall= recall_score(y_true_test, predict)\n",
        "\n",
        "auc_list.append(auc)\n",
        "ap_list.append(precision)\n",
        "ar_list.append(recall)\n",
        "af1_list.append(2*precision*recall/(precision+recall))\n",
        "\n",
        "print('----------------------------------------------------------------')\n",
        "print('Link Prediction Tests Summary')\n",
        "print('AUC_mean = {}'.format(np.mean(auc_list)))\n",
        "print('AP_mean = {}'.format(np.mean(ap_list)))\n",
        "print('AR_mean = {}'.format(np.mean(ar_list)))\n",
        "print('AF1_mean = {},'.format(np.mean(af1_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qF0ge1idnZi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle,pathlib,scipy.sparse\n",
        "\n",
        "basic_repo='/content/drive/MyDrive/facebook_react'\n",
        "save_path='/content/drive/MyDrive/facebook_react/processed/'\n",
        "\n",
        "# user_repo=pd.read_csv(basic_repo+'/edge_index/user_repo_index.txt', sep='\\t', header=None, names=['user_id', 'repo_id'], keep_default_na=False, encoding='utf-8')\n",
        "# user_issue=pd.read_csv(basic_repo+'/edge_index/user_issue_index.txt', sep='\\t', header=None, names=['user_id', 'issue_id'], keep_default_na=False, encoding='utf-8')\n",
        "# user_pr=pd.read_csv(basic_repo+'/edge_index/user_pr_index.txt', sep='\\t', header=None, names=['user_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "# repo_repo = pd.read_csv(basic_repo+'/edge_index/repo_repo_index.txt', sep='\\t', header=None, names=['repo_id', 'repo_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "# repo_issue = pd.read_csv(basic_repo+'/edge_index/repo_issue_index.txt', sep='\\t', header=None, names=['repo_id', 'issue_id'], keep_default_na=False, encoding='utf-8')\n",
        "# repo_pr = pd.read_csv(basic_repo+'/edge_index/repo_pr_index.txt', sep='\\t', header=None, names=['repo_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "# issue_issue=pd.read_csv(basic_repo+'/edge_index/issue_issue_index.txt', sep='\\t', header=None, names=['issue_id', 'issue_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "# # issue_pr=pd.read_csv(basic_repo+'/edge_index/issue_pr_index.txt', sep='\\t', header=None, names=['issue_id', 'pr_id'], keep_default_na=False, encoding='utf-8')\n",
        "# pr_pr=pd.read_csv(basic_repo+'/edge_index/pr_pr_index.txt', sep='\\t', header=None, names=['pr_id', 'pr_id_1'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "# issues = pd.read_csv(basic_repo+'/index/issue_index.txt', sep='\\t', header=None, names=['issue_id', 'issue'], keep_default_na=False, encoding='utf-8')\n",
        "# prs = pd.read_csv(basic_repo+'/index/pr_index.txt', sep='\\t', header=None, names=['pr_id', 'pr'], keep_default_na=False, encoding='utf-8')\n",
        "# users = pd.read_csv(basic_repo+'/index/user_index.txt', sep='\\t', header=None, names=['user_id', 'user'], keep_default_na=False, encoding='utf-8')\n",
        "# repos= pd.read_csv(basic_repo+'/index/repo_index.txt', sep='\\t', header=None, names=['repo_id', 'repo'], keep_default_na=False, encoding='utf-8')\n",
        "\n",
        "# num_users=len(users)\n",
        "# num_repos=len(repos)\n",
        "# num_issues=len(issues)\n",
        "# num_prs=len(prs)\n",
        "\n",
        "# dim = len(users) + len(repos) + len(issues) + len(prs)\n",
        "\n",
        "# ########### 保存每个节点的type all nodes (users, repos, issues and prs) type labels\n",
        "# type_mask = np.zeros((dim), dtype=int)\n",
        "# type_mask[len(users):len(users)+len(repos)] = 1\n",
        "# type_mask[len(users)+len(repos):len(users)+len(repos)+len(issues)] = 2\n",
        "# type_mask[len(users)+len(repos)+len(issues):] = 3\n",
        "\n",
        "\n",
        "# #将每一个节点对应 的标号映射到列表的位置上\n",
        "# user_id_mapping = {row['user_id']: i for i, row in users.iterrows()}\n",
        "# repo_id_mapping = {row['repo_id']: i + len(users) for i, row in repos.iterrows()}\n",
        "# issue_id_mapping = {row['issue_id']: i + len(users)+len(repos) for i, row in issues.iterrows()}\n",
        "# pr_id_mapping = {row['pr_id']: i + len(users)+len(repos)+len(issues) for i, row in prs.iterrows()}\n",
        "\n",
        "####### 构建邻接矩阵\n",
        "\n",
        "# adjM_ur = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in user_repo.iterrows():\n",
        "#     idx1 = user_id_mapping[row['user_id']]\n",
        "#     idx2 = repo_id_mapping[row['repo_id']]\n",
        "#     adjM_ur[idx1, idx2] = 1\n",
        "# adjM_ur=scipy.sparse.csr_matrix(adjM_ur)\n",
        "# adjM_ru = adjM_ur.transpose()\n",
        "\n",
        "# adjM_ui = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in user_issue.iterrows():\n",
        "#     idx1 = user_id_mapping[row['user_id']]\n",
        "#     idx2 = issue_id_mapping[row['issue_id']]\n",
        "#     adjM_ui[idx1, idx2] = 1\n",
        "# adjM_iu=adjM_ui.transpose()\n",
        "\n",
        "# adjM_up = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in user_pr.iterrows():\n",
        "#     idx1 = user_id_mapping[row['user_id']]\n",
        "#     idx2 = pr_id_mapping[row['pr_id']]\n",
        "#     adjM_up[idx1, idx2] = 1\n",
        "# adjM_pu = adjM_up.transpose()\n",
        "\n",
        "# adjM_rr = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in repo_repo.iterrows():\n",
        "#     idx1 = repo_id_mapping[row['repo_id']]\n",
        "#     idx2 = repo_id_mapping[row['repo_id_1']]\n",
        "#     adjM_rr[idx1, idx2] = 1\n",
        "# adjM_rr_t = adjM_rr.transpose()\n",
        "\n",
        "# adjM_ri = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in repo_issue.iterrows():\n",
        "#     idx1 = repo_id_mapping[row['repo_id']]\n",
        "#     idx2 = issue_id_mapping[row['issue_id']]\n",
        "#     adjM_ri[idx1, idx2] = 1\n",
        "# adjM_ir=adjM_ri.transpose()\n",
        "\n",
        "# adjM_rp = np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in repo_pr.iterrows():\n",
        "#     idx1 = repo_id_mapping[row['repo_id']]\n",
        "#     idx2 = pr_id_mapping[row['pr_id']]\n",
        "#     adjM_rp[idx1, idx2] = 1\n",
        "# adjM_pr=adjM_rp.transpose()\n",
        "\n",
        "# adjM_ii=np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in issue_issue.iterrows():\n",
        "#     idx1 = issue_id_mapping[row['issue_id']]\n",
        "#     idx2 = issue_id_mapping[row['issue_id_1']]\n",
        "#     adjM_ii[idx1, idx2] = 1\n",
        "# adjM_ii_t=adjM_ii.transpose()\n",
        "\n",
        "# adjM_pp=np.zeros((dim, dim), dtype=int)\n",
        "# for _, row in pr_pr.iterrows():\n",
        "#     idx1 = pr_id_mapping[row['pr_id']]\n",
        "#     idx2 = pr_id_mapping[row['pr_id_1']]\n",
        "#     adjM_pp[idx1, idx2] = 1\n",
        "# adjM_pp_t=adjM_pp.transpose()\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ur.npz', scipy.sparse.csr_matrix(adjM_ur))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ru.npz', scipy.sparse.csr_matrix(adjM_ru))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ui.npz', scipy.sparse.csr_matrix(adjM_ui))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_iu.npz', scipy.sparse.csr_matrix(adjM_iu))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_up.npz', scipy.sparse.csr_matrix(adjM_up))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_pu.npz', scipy.sparse.csr_matrix(adjM_pu))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_rr.npz', scipy.sparse.csr_matrix(adjM_rr))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_rr_t.npz', scipy.sparse.csr_matrix(adjM_rr_t))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ri.npz', scipy.sparse.csr_matrix(adjM_ri))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ir.npz', scipy.sparse.csr_matrix(adjM_ir))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_rp.npz', scipy.sparse.csr_matrix(adjM_rp))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_pr.npz', scipy.sparse.csr_matrix(adjM_pr))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ii.npz', scipy.sparse.csr_matrix(adjM_ii))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_ii_t.npz', scipy.sparse.csr_matrix(adjM_ii_t))\n",
        "\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_pp.npz', scipy.sparse.csr_matrix(adjM_pp))\n",
        "# scipy.sparse.save_npz(save_path + 'adjM_pp_t.npz', scipy.sparse.csr_matrix(adjM_pp_t))\n",
        "adjM_ur = scipy.sparse.load_npz(save_path + 'adjM_ur.npz')\n",
        "adjM_ur=adjM_ur.toarray()\n",
        "# adjM_ru = scipy.sparse.load_npz(save_path + 'adjM_ru.npz')\n",
        "# adjM_ru=adjM_ru.toarray()\n",
        "# adjM_ui = scipy.sparse.load_npz(save_path + 'adjM_ui.npz')\n",
        "# adjM_ui= adjM_ui.toarray()\n",
        "# adjM_iu = scipy.sparse.load_npz(save_path + 'adjM_iu.npz')\n",
        "# adjM_iu=adjM_iu.toarray()\n",
        "# adjM_up = scipy.sparse.load_npz(save_path + 'adjM_up.npz')\n",
        "# adjM_up= adjM_up.toarray()\n",
        "# adjM_pu = scipy.sparse.load_npz(save_path + 'adjM_pu.npz')\n",
        "# adjM_pu=adjM_pu.toarray()\n",
        "\n",
        "edges=[adjM_ur]\n",
        "      #adjM_ru]\n",
        "      # adjM_ui,adjM_iu]\n",
        "      # adjM_up,adjM_pu]\n",
        "      #  adjM_rr,adjM_rr_t]\n",
        "      #  adjM_ri,adjM_ir,adjM_rp,adjM_pr,adjM_ii,adjM_ii_t,adjM_pp,adjM_pp_t]\n",
        "\n",
        "with open(basic_repo+'/edges.pkl', 'wb') as f:\n",
        "    pickle.dump(edges, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}